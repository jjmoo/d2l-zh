{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全卷积网络（FCN）\n",
    "\n",
    "上一节介绍了，我们可以基于语义分割对图像中的每个像素进行类别预测。全卷积网络（fully convolutional network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换 [1]。与之前介绍的卷积神经网络有所不同，全卷积网络通过转置卷积（transposed convolution）层将中间层特征图的高和宽变换回输入图像的尺寸，从而令预测结果与输入图像在空间维（高和宽）上一一对应：给定空间维上的位置，通道维的输出即该位置对应像素的类别预测。\n",
    "\n",
    "我们先导入实验所需的包或模块，然后解释什么是转置卷积层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon, image, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, model_zoo, nn\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转置卷积层\n",
    "\n",
    "顾名思义，转置卷积层得名于矩阵的转置操作。事实上，卷积运算还可以通过矩阵乘法来实现。在下面的例子中，我们定义高和宽分别为4的输入`X`，以及高和宽分别为3的卷积核`K`。打印二维卷积运算的输出以及卷积核。可以看到，输出的高和宽分别为2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nd.arange(1, 17).reshape((1, 1, 4, 4))\n",
    "K = nd.arange(1, 10).reshape((1, 1, 3, 3))\n",
    "conv = nn.Conv2D(channels=1, kernel_size=3)\n",
    "conv.initialize(init.Constant(K))\n",
    "conv(X), K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们将卷积核`K`改写成含有大量零元素的稀疏矩阵`W`，即权重矩阵。权重矩阵的形状为(4, 16)，其中的非零元素来自卷积核`K`中的元素。将输入`X`逐行连结，得到长度为16的向量。然后将`W`与向量化的`X`做矩阵乘法，得到长度为4的向量。对其变形后，我们可以得到和上面卷积运算相同的结果。可见，我们在这个例子中使用矩阵乘法实现了卷积运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, k = nd.zeros((4, 16)), nd.zeros(11)\n",
    "k[:3], k[4:7], k[8:] = K[0, 0, 0, :], K[0, 0, 1, :], K[0, 0, 2, :]\n",
    "W[0, 0:11], W[1, 1:12], W[2, 4:15], W[3, 5:16] = k, k, k, k\n",
    "nd.dot(W, X.reshape(16)).reshape((1, 1, 2, 2)), W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们从矩阵乘法的角度来描述卷积运算。设输入向量为$\\boldsymbol{x}$，权重矩阵为$\\boldsymbol{W}$，卷积的前向计算函数的实现可以看作将函数输入乘以权重矩阵，并输出向量$\\boldsymbol{y} = \\boldsymbol{W}\\boldsymbol{x}$。我们知道，反向传播需要依据链式法则。由于$\\nabla_{\\boldsymbol{x}} \\boldsymbol{y} = \\boldsymbol{W}^\\top$，卷积的反向传播函数的实现可以看作将函数输入乘以转置后的权重矩阵$\\boldsymbol{W}^\\top$。而转置卷积层正好交换了卷积层的前向计算函数与反向传播函数：转置卷积层的这两个函数可以看作将函数输入向量分别乘以$\\boldsymbol{W}^\\top$和$\\boldsymbol{W}$。\n",
    "\n",
    "不难想象，转置卷积层可以用来交换卷积层输入和输出的形状。让我们继续用矩阵乘法描述卷积。设权重矩阵是形状为$4\\times16$的矩阵，对于长度为16的输入向量，卷积前向计算输出长度为4的向量。假如输入向量的长度为4，转置权重矩阵的形状为$16\\times4$，那么转置卷积层将输出长度为16的向量。在模型设计中，转置卷积层常用于将较小的特征图变换为更大的特征图。在全卷积网络中，当输入是高和宽较小的特征图时，转置卷积层可以用来将高和宽放大到输入图像的尺寸。\n",
    "\n",
    "我们来看一个例子。构造一个卷积层`conv`，并设输入`X`的形状为(1, 3, 64, 64)。卷积输出`Y`的通道数增加到10，但高和宽分别缩小了一半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "conv = nn.Conv2D(10, kernel_size=4, padding=1, strides=2)\n",
    "conv.initialize()\n",
    "\n",
    "X = nd.random.uniform(shape=(1, 3, 64, 64))\n",
    "Y = conv(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们通过创建`Conv2DTranspose`实例来构造转置卷积层`conv_trans`。这里我们设`conv_trans`的卷积核形状、填充以及步幅与`conv`中的相同，并设输出通道数为3。当输入为卷积层`conv`的输出`Y`时，转置卷积层输出与卷积层输入的高和宽相同：转置卷积层将特征图的高和宽分别放大了2倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "conv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2)\n",
    "conv_trans.initialize()\n",
    "conv_trans(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在有些文献中，转置卷积也被称为分数步长卷积（fractionally-strided convolution）[2]。\n",
    "\n",
    "\n",
    "## 构造模型\n",
    "\n",
    "我们在这里给出全卷积网络模型最基本的设计。如图9.11所示，全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。模型输出与输入图像的高和宽相同，并在空间位置上一一对应：最终输出的通道包含了该空间位置像素的类别预测。\n",
    "\n",
    "![全卷积网络](../img/fcn.svg)\n",
    "\n",
    "下面我们使用一个基于ImageNet数据集预训练的ResNet-18模型来抽取图像特征，并将该网络实例记为`pretrained_net`。可以看到，该模型成员变量`features`的最后两层分别是全局最大池化层`GlobalAvgPool2D`和样本变平层`Flatten`，而`output`模块包含了输出用的全连接层。全卷积网络不需要使用这些层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_net = model_zoo.vision.resnet18_v2(pretrained=True)\n",
    "pretrained_net.features[-4:], pretrained_net.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们创建全卷积网络实例`net`。它复制了`pretrained_net`实例成员变量`features`里除去最后两层的所有层以及预训练得到的模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "for layer in pretrained_net.features[:-2]:\n",
    "    net.add(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定高和宽分别为320和480的输入，`net`的前向计算将输入的高和宽减小至原来的$1/32$，即10和15。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "X = nd.random.uniform(shape=(1, 3, 320, 480))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们通过$1\\times 1$卷积层将输出通道数变换为Pascal VOC2012数据集的类别个数21。最后，我们需要将特征图的高和宽放大32倍，从而变回输入图像的高和宽。回忆一下[“填充和步幅”](../chapter_convolutional-neural-networks/padding-and-strides.ipynb)一节中描述的卷积层输出形状的计算方法。由于$(320-64+16\\times2+32)/32=10$且$(480-64+16\\times2+32)/32=15$，我们构造一个步幅为32的转置卷积层，并将卷积核的高和宽设为64、填充设为16。不难发现，如果步幅为$s$、填充为$s/2$（假设$s/2$为整数）、卷积核的高和宽为$2s$，转置卷积核将输入的高和宽分别放大$s$倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 21\n",
    "net.add(nn.Conv2D(num_classes, kernel_size=1),\n",
    "        nn.Conv2DTranspose(num_classes, kernel_size=64, padding=16,\n",
    "                           strides=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化转置卷积层\n",
    "\n",
    "我们已经知道，转置卷积层可以放大特征图。在图像处理中，我们有时需要将图像放大，即上采样（upsample）。上采样的方法有很多，常用的有双线性插值。简单来说，为了得到输出图像在坐标$(x,y)$上的像素，先将该坐标映射到输入图像的坐标$(x',y')$，例如，根据输入与输出的尺寸之比来映射。映射后的$x'$和$y'$通常是实数。然后，在输入图像上找到与坐标$(x',y')$最近的4个像素。最后，输出图像在坐标$(x,y)$上的像素依据输入图像上这4个像素及其与$(x',y')$的相对距离来计算。双线性插值的上采样可以通过由以下`bilinear_kernel`函数构造的卷积核的转置卷积层来实现。限于篇幅，我们只给出`bilinear_kernel`函数的实现，不再讨论算法的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype='float32')\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return nd.array(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来实验一下用转置卷积层实现的双线性插值的上采样。构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用`bilinear_kernel`函数初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "conv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2)\n",
    "conv_trans.initialize(init.Constant(bilinear_kernel(3, 3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取图像`X`，将上采样的结果记作`Y`。为了打印图像，我们需要调整通道维的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.imread('../img/catdog.jpg')\n",
    "X = img.astype('float32').transpose((2, 0, 1)).expand_dims(axis=0) / 255\n",
    "Y = conv_trans(X)\n",
    "out_img = Y[0].transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，转置卷积层将图像的高和宽分别放大2倍。值得一提的是，除了坐标刻度不同，双线性插值放大的图像和[“目标检测和边界框”](bounding-box.ipynb)一节中打印出的原图看上去没什么两样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "print('input image shape:', img.shape)\n",
    "d2l.plt.imshow(img.asnumpy());\n",
    "print('output image shape:', out_img.shape)\n",
    "d2l.plt.imshow(out_img.asnumpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在全卷积网络中，我们将转置卷积层初始化为双线性插值的上采样。对于$1\\times 1$卷积层，我们采用Xavier随机初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "net[-1].initialize(init.Constant(bilinear_kernel(num_classes, num_classes,\n",
    "                                                 64)))\n",
    "net[-2].initialize(init=init.Xavier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集\n",
    "\n",
    "我们用上一节介绍的方法读取数据集。这里指定随机裁剪的输出图像的形状为$320\\times 480$：高和宽都可以被32整除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "crop_size, batch_size, colormap2label = (320, 480), 32, nd.zeros(256**3)\n",
    "for i, cm in enumerate(d2l.VOC_COLORMAP):\n",
    "    colormap2label[(cm[0] * 256 + cm[1]) * 256 + cm[2]] = i\n",
    "voc_dir = d2l.download_voc_pascal(data_dir='../data')\n",
    "\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "train_iter = gdata.DataLoader(\n",
    "    d2l.VOCSegDataset(True, crop_size, voc_dir, colormap2label), batch_size,\n",
    "    shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "test_iter = gdata.DataLoader(\n",
    "    d2l.VOCSegDataset(False, crop_size, voc_dir, colormap2label), batch_size,\n",
    "    last_batch='discard', num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "现在可以开始训练模型了。这里的损失函数和准确率计算与图像分类中的并没有本质上的不同。因为我们使用转置卷积层的通道来预测像素的类别，所以在`SoftmaxCrossEntropyLoss`里指定了`axis=1`（通道维）选项。此外，模型基于每个像素的预测类别是否正确来计算准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "ctx = d2l.try_all_gpus()\n",
    "loss = gloss.SoftmaxCrossEntropyLoss(axis=1)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1,\n",
    "                                                      'wd': 1e-3})\n",
    "d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测像素类别\n",
    "\n",
    "在预测时，我们需要将输入图像在各个通道做标准化，并转成卷积神经网络所需要的四维输入格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    X = test_iter._dataset.normalize_image(img)\n",
    "    X = X.transpose((2, 0, 1)).expand_dims(axis=0)\n",
    "    pred = nd.argmax(net(X.as_in_context(ctx[0])), axis=1)\n",
    "    return pred.reshape((pred.shape[1], pred.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了可视化每个像素的预测类别，我们将预测类别映射回它们在数据集中的标注颜色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "def label2image(pred):\n",
    "    colormap = nd.array(d2l.VOC_COLORMAP, ctx=ctx[0], dtype='uint8')\n",
    "    X = pred.astype('int32')\n",
    "    return colormap[X, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据集中的图像大小和形状各异。由于模型使用了步幅为32的转置卷积层，当输入图像的高或宽无法被32整除时，转置卷积层输出的高或宽会与输入图像的尺寸有偏差。为了解决这个问题，我们可以在图像中截取多块高和宽为32的整数倍的矩形区域，并分别对这些区域中的像素做前向计算。这些区域的并集需要完整覆盖输入图像。当一个像素被多个区域所覆盖时，它在不同区域前向计算中转置卷积层输出的平均值可以作为softmax运算的输入，从而预测类别。\n",
    "\n",
    "简单起见，我们只读取几张较大的测试图像，并从图像的左上角开始截取形状为$320\\times480$的区域：只有该区域用来预测。对于输入图像，我们先打印截取的区域，再打印预测结果，最后打印标注的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "test_images, test_labels = d2l.read_voc_images(is_train=False)\n",
    "n, imgs = 4, []\n",
    "for i in range(n):\n",
    "    crop_rect = (0, 0, 480, 320)\n",
    "    X = image.fixed_crop(test_images[i], *crop_rect)\n",
    "    pred = label2image(predict(X))\n",
    "    imgs += [X, pred, image.fixed_crop(test_labels[i], *crop_rect)]\n",
    "d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 可以通过矩阵乘法来实现卷积运算。\n",
    "* 全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸，从而输出每个像素的类别。\n",
    "* 在全卷积网络中，可以将转置卷积层初始化为双线性插值的上采样。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 用矩阵乘法来实现卷积运算是否高效？为什么？\n",
    "* 如果将转置卷积层改用Xavier随机初始化，结果有什么变化？\n",
    "* 调节超参数，能进一步提升模型的精度吗？\n",
    "* 预测测试图像中所有像素的类别。\n",
    "* 全卷积网络的论文中还使用了卷积神经网络的某些中间层的输出 [1]。试着实现这个想法。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "[1] Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431-3440).\n",
    "\n",
    "[2] Dumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285.\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/3041)\n",
    "\n",
    "![](../img/qr_fcn.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
